---
title: "Learning Interpretable Characteristic Kernels via Decision Forests"
authors: "<strong>Sambit Panda</strong>*, Cencheng Shen*, and Joshua T. Vogelstein"
journal: arXiv
doi: "10.48550/arXiv.1812.00029"
abstract: |
  Decision forests are widely used for classification and regression tasks. A lesser known property of tree-based methods is that one can construct a proximity matrix from the tree(s), and these proximity matrices are induced kernels. While there has been extensive research on the applications and properties of kernels, there is relatively little research on kernels induced by decision forests. We construct Kernel Mean Embedding Random Forests (KMERF), which induce kernels from random trees and/or forests using leaf-node proximity. We introduce the notion of an asymptotically characteristic kernel, and prove that KMERF kernels are asymptotically characteristic for both discrete and continuous data. Because KMERF is data-adaptive, we suspected it would outperform kernels selected a priori on finite sample data. We illustrate that KMERF nearly dominates current state-of-the-art kernel-based tests across a diverse range of high-dimensional two-sample and independence testing settings. Furthermore, our forest-based approach is interpretable, and provides feature importance metrics that readily distinguish important dimensions, unlike other high-dimensional non-parametric testing procedures. Hence, this work demonstrates the decision forest-based kernel can be more powerful and more interpretable than existing methods, flying in the face of conventional wisdom of the trade-off between the two.
links:
  - "Code: https://hyppo.neurodata.io/api/generated/hyppo.independence.kmerf#hyppo.independence.KMERF"
  - "Talk: ../assets/pdf/2023-kmerf-cis-retreat.pdf"
type: preprint
date: 2023-09-29
featured: true
equalContribution: true
intro: Demonstrates the kernel derived from random forest is characteristic and develops a hypothesis test based on that fact (KMERF).
---
